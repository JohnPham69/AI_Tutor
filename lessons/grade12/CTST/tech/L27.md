## BÀI F16: MÁY TÍNH, THUẬT TOÁN VÀ KHOA HỌC DỮ LIỆU

### MỤC TIÊU

**Sau bài học này, em sẽ:**

* Biết được vai trò của máy tính đối với sự phát triển của Khoa học dữ liệu.

* Biết được tính ưu việt trong việc sử dụng máy tính và thuật toán hiệu quả để xử lí dữ liệu lớn, nêu được ví dụ minh hoạ.

### KHỞI ĐỘNG

Theo em, nhà trường thường thu thập, lưu trữ và xử lí những dữ liệu nào về học sinh?

### KHÁM PHÁ

#### 1. Vai trò của máy tính đối với sự phát triển của Khoa học dữ liệu

Sự phát triển của máy tính đóng vai trò quan trọng trong việc cung cấp nền tảng cho sự phát triển của Khoa học dữ liệu. Năng lực tính toán mạnh mẽ, lưu trữ lớn, tốc độ xử lí cao, xử lí đa nhiệm và kết nối mạng là các yếu tố quan trọng giúp nâng cao khả năng phân tích và hiểu biết dữ liệu lớn. Các thành tựu của phần cứng máy tính như: bộ xử lí đồ hoạ (Graphics Processing Unit-GPU), bộ xử lí tensor (Tensor Processing Unit-TPU), điện toán đám mây, máy tính lượng tử,... đã giúp Khoa học dữ liệu phát triển nhanh chóng và đạt hiệu quả cao hơn trong việc phân tích xử lí dữ liệu lớn. Quan sát đặc điểm của máy tính và Khoa học dữ liệu qua các thời kì, em có thể thấy được vai trò quan trọng của máy tính trong quá trình phát triển của Khoa học dữ liệu.

**Bảng 1. Đặc điểm của máy tính đối với sự phát triển của Khoa học dữ liệu qua các thời kì**

| Giai đoạn | Đặc điểm của máy tính | Khoa học dữ liệu |
|---|---|---|
| 1945 – 1955 | Máy tính không có hệ điều hành và được dùng để thực hiện các tính toán đơn giản. | Sử dụng các phương pháp thống kê để phân tích tập dữ liệu nhỏ. |
| 1955 – 1965 | Máy tính có hệ điều hành và xuất hiện ngôn ngữ lập trình. | Thống kê máy tính được áp dụng cho lượng dữ liệu lớn hơn. Máy tính được sử dụng để hỗ trợ, tính toán và phân tích dữ liệu. |
| 1965 – 1980 | Máy tính có khả năng xử lí đa nhiệm. | Sự tăng trưởng về số lượng và độ phức tạp của dữ liệu đòi hỏi sử dụng các kĩ thuật xử lí dữ liệu, phương pháp thống kê nâng cao hơn. |
| 1980 – 1990 | Máy tính cá nhân trở nên phổ biến, Internet phát triển, bắt đầu lưu trữ dữ liệu trên mạng. | Sự phát triển của các phương pháp học máy. Khoa học dữ liệu tập trung vào việc xử lí và phân tích dữ liệu lớn. |
| 1990 – 2000 | Internet và thiết bị di động trở nên phổ biến, tạo ra lượng lớn dữ liệu từ các nguồn khác nhau. | Sự phát triển mạnh mẽ của Học máy và Khoa học dữ liệu, Dữ liệu lớn. |
| 2000 – đến nay | Điện toán đám mây. Sự xuất hiện của GPU, TPU giúp tăng cường tốc độ và hiệu suất tính toán. | Học máy và Trí tuệ nhân tạo trở nên phổ biến, giúp tăng cường hiểu biết dữ liệu và tự động hoá. Các mô hình học máy tận dụng GPU, TPU để học và giải quyết các vấn đề phức tạp trong Khoa học dữ liệu. |

Hiện nay, các dự án liên quan đến Khoa học dữ liệu thường áp dụng một quy trình chung, được gọi là quy trình Khoa học dữ liệu. Bao gồm một số giai đoạn như:

1. **Xác định vấn đề:** Đặt ra câu hỏi cần trả lời hoặc mục tiêu cụ thể cần giải quyết của dự án.

2. **Thu thập dữ liệu:** Tìm kiếm, thu thập hoặc tạo dữ liệu cần thiết để giải quyết vấn đề. Dữ liệu thu thập thường được chia làm hai phần, phần thứ nhất (gọi là dữ liệu huấn luyện) dùng cho mô hình học máy học, phần còn lại (gọi là dữ liệu kiểm tra) dùng để đánh giá hiệu suất của mô hình.

3. **Chuẩn bị dữ liệu:** Làm sạch dữ liệu, khám phá tri thức ẩn trong dữ liệu, trực quan hoá dữ liệu và chuẩn bị cho việc xây dựng mô hình học máy.

4. **Xây dựng mô hình:** Chọn và áp dụng mô hình học máy phù hợp với dự án.

5. **Đánh giá:** Đánh giá hiệu suất của mô hình với dữ liệu kiểm tra đã có ở bước thu thập dữ liệu.

6. **Triển khai:** Triển khai mô hình vào sử dụng trong môi trường thực tế.

Có thể thấy rằng, máy tính có vai trò quan trọng trong mọi giai đoạn của quy trình Khoa học dữ liệu. Máy tính được sử dụng để lưu trữ, xử lí và phân tích dữ liệu. Các mô hình học máy và khai phá dữ liệu được thực hiện trên các máy tính để tìm kiếm các mẫu và thông tin tiềm ẩn trong dữ liệu. Máy tính cũng được sử dụng để tạo ra các mô hình dự đoán và phân loại. Vai trò của máy tính đối với Khoa học dữ liệu được thể hiện ở Bảng 2.

**Bảng 2. Vai trò của máy tính đối với Khoa học dữ liệu**

| Nội dung | Vai trò của máy tính | Khoa học dữ liệu |
|---|---|---|
| Lưu trữ và xử lí dữ liệu | Dung lượng lưu trữ của phần cứng ảnh hưởng đến khả năng xử lí dữ liệu lớn. | Khoa học dữ liệu yêu cầu lưu trữ hiệu quả để quản lí và truy xuất dữ liệu. |
| Tính toán và trực quan hoá | Phần cứng máy tính cung cấp sức mạnh tính toán nhanh chóng và hiệu quả. | Khoa học dữ liệu sử dụng tính toán để phân tích, xử lí và biểu diễn dữ liệu lớn. |
| Điện toán đám mây | Phần cứng mạng ảnh hưởng đến khả năng kết nối và truyền tải dữ liệu, hỗ trợ tính toán phân tán và triển khai trên nhiều máy cùng lúc. | Khoa học dữ liệu sử dụng Internet, điện toán đám mây để lưu trữ và quản lí khối lượng lớn dữ liệu, cho phép sử dụng nguồn lực tính toán mạnh mẽ để xử lí dữ liệu lớn. |

Theo em, điện toán đám mây có vai trò như thế nào trong Khoa học dữ liệu?

Máy tính đóng vai trò quan trọng trong mọi giai đoạn của quy trình Khoa học dữ liệu. Máy tính cung cấp khả năng lưu trữ, sức mạnh tính toán để xử lí, khám phá tri thức, phân tích dữ liệu. Máy tính với khả năng kết nối và điện toán đám mây cho phép xử lí dữ liệu lớn và đa dạng, góp phần vào quá trình phát triển của Khoa học dữ liệu.

#### 2. Tính ưu việt của sử dụng máy tính và thuật toán trong xử lí dữ liệu lớn

Dữ liệu lớn và Khoa học dữ liệu có mối quan hệ mật thiết. Khoa học dữ liệu sử dụng các kĩ thuật và công cụ để phân tích, khám phá, trích rút thông tin từ dữ liệu lớn. Trong khi đó, dữ liệu lớn cung cấp nguồn nguyên liệu phong phú để áp dụng các phương pháp và kĩ thuật trong Khoa học dữ liệu. Thuật ngữ dữ liệu lớn (Big Data) xuất hiện từ những năm 1990 và được sử dụng để mô tả quy mô lớn và phức tạp của dữ liệu, cả về khối lượng và sự đa dạng mà không thể xử lí bằng cách sử dụng phương pháp truyền thống. Dữ liệu lớn bao gồm năm yếu tố chính, thường được gọi là 5V, bao gồm: Khối lượng (Volume) đề cập tới khối lượng dữ liệu rất lớn; Tốc độ (Velocity) đề cập tới dữ liệu được tạo ra rất nhanh; Đa dạng (Variety) đề cập tới các loại dữ liệu khác nhau, bao gồm dữ liệu có cấu trúc (cơ sở dữ liệu quan hệ), bán cấu trúc (XML, JSON) và phi cấu trúc (email, bài đăng trên mạng xã hội, âm thanh, hình ảnh, video); Độ tin cậy hay độ xác thực (Veracity) đề cập đến độ tin cậy và chất lượng của dữ liệu; Giá trị (Value) đề cập tới giá trị mà dữ liệu mang lại.

Máy tính đóng vai trò quan trọng trong mọi giai đoạn của quy trình Khoa học dữ liệu, bao gồm yêu cầu xử lí dữ liệu lớn. Sử dụng máy tính để xử lí dữ liệu lớn mang lại nhiều ưu điểm đáng kể, từ tốc độ xử lí đến khả năng linh hoạt và đa nhiệm, giúp tăng cường khả năng phân tích, rút trích thông tin, dự báo,... Sử dụng máy tính để xử lí dữ liệu lớn mang lại nhiều ưu việt, thể hiện ở các nội dung sau:

**Tốc độ xử lí:** Với khả năng xử lí hàng triệu hoặc thậm chí hàng tỉ phép tính mỗi giây, cho phép máy tính xử lí dữ liệu lớn nhanh chóng, giúp tiết kiệm thời gian so với thực hiện công việc thủ công hoặc bằng các phương pháp truyền thống.

Thực hiện các mô hình học máy trên dữ liệu lớn: Máy tính được sử dụng để xây dựng và triển khai các mô hình học máy xử lí trên dữ liệu lớn. Em có thể kiểm thử cũng như điều chỉnh tính hiệu quả và độ chính xác của mô hình học máy trên tập dữ liệu thử nghiệm lớn và đa dạng trên máy tính.

**Khả năng mở rộng:** Máy tính có thể được kết hợp thành các mạng máy tính, giúp xử lí dữ liệu ở quy mô lớn hơn.

**Lưu trữ và bảo mật dữ liệu:** Máy tính cung cấp khả năng lưu trữ và truy xuất dữ liệu lớn từ những nguồn khác nhau, bao gồm lưu trữ trực tuyến. Ngoài ra, máy tính cung cấp các công cụ để bảo vệ dữ liệu và kiểm soát quyền truy cập, đảm bảo tính an toàn. Máy tính có thể xử lí dữ liệu có cấu trúc và phi cấu trúc, dữ liệu văn bản, hình ảnh âm thanh và dữ liệu thời gian thực.

**Xử lí theo thời gian thực:** Máy tính cho phép xử lí và phân tích dữ liệu lớn theo thời gian thực. Điều này rất quan trọng trong các ứng dụng như giám sát quy trình công nghiệp, giao dịch tài chính,...

Trong xử lí dữ liệu lớn, bên cạnh việc sử dụng máy tính, các thuật toán cũng được sử dụng để tăng cường khả năng xử lí, thể hiện ở những nội dung sau:

**Xử lí song song:** Các kĩ thuật dựa trên thuật toán song song cho phép mô hình hình thực hiện trên nhiều nguồn tính toán, tận dụng sức mạnh của máy tính đa nhân, hệ thống kết nối nhiều máy tính, điện toán đám mây để giảm thời gian và chi phí trong quá trình xử lí dữ liệu.

**Tự động hoá:** Thuật toán giúp tự động hoá nhiều quy trình xử lí dữ liệu, giảm bớt sự phụ thuộc vào các quá trình thủ công. Ví dụ, sử dụng các thuật toán để tự động hoá các nhiệm vụ có tính chất lặp lại như làm sạch dữ liệu, đồng bộ hoá dữ liệu giữa các nguồn lưu trữ khác nhau.

Ví dụ 1: Vào năm 2020, GPT-3 của OpenAI đã thu thập khoảng 45 tỉ dữ liệu thô từ sách, tạp chí, trang web,... với nhiều chủ đề khác nhau. Sau đó, dữ liệu thô được xử lí để tạo ra 570 GB dữ liệu vào cho mô hình học máy¹. Lượng dữ liệu thô thu thập dùng cho GPT-3 tương đương khoảng 200 000 giờ video full HD hoặc 15 triệu giờ âm thanh định dạng MP3. Các phiên bản khác nhau của GPT-3 sử dụng từ 125 triệu đến 175 tỉ tham số (phụ thuộc vào số thông số trong mạng học sâu của mô hình). Theo thời gian, việc đào tạo mô hình GPT-3 với 175 tỉ tham số cần hàng trăm năm với 1 GPU V100. Để giảm thời huấn luyện mô hình học máy, OpenAI đã sử dụng 1024 GPU NVIDIA A100 để huấn luyện mô hình GPT-3 trong 34 ngày².

Ví dụ 2: Protein là nhân tố thiết yếu cho sự sống được hình thành từ các amino axit, sau đó trải qua quá trình gấp cuộn để hình thành cấu trúc 3D phức tạp. Chức năng của protein phụ thuộc chủ yếu vào cấu trúc 3D của nó. Trong nhiều thập kỉ, các nhà nghiên cứu đã giải mã cấu trúc 3D của protein bằng cách sử dụng các kĩ thuật như tinh thể học tia X (X-ray crystallography) hoặc hiển vi điện tử lạnh (Cryogenic electron microscopy viết tắt là Cryo-EM) và đã giải được cấu trúc của 170 000 protein trong số khoảng 200 triệu protein. Theo ước tính, để dự đoán cấu trúc một protein dựa trên phương pháp tinh thể học tia X cần vài tháng đến hàng năm.

Vào năm 2020, DeepMind đã phát triển mô hình học máy AlphaFold 2 có khả năng dự đoán cấu trúc protein. Với độ chính xác cao trong khoảng thời gian từ vài giờ đến vài ngày tuỳ thuộc vào độ phức tạp của protein, cấu hình máy tính và kích thước của mô hình dự đoán. AlphaFold 2 cũng đã được sử dụng để dự đoán cấu trúc protein của virut SARS-CoV-2.

Vào năm 2023, các nhà khoa học tại Viện Tin sinh học châu Âu thuộc EMBL (EMBL-EBI), Viện Công nghệ Thuỵ Sĩ (ETH Zurich) và Google DeepMind đã sử dụng tập dữ liệu AlphaFold DB (chứa khoảng 200 triệu cấu trúc protein dự đoán dựa trên mô hình học máy) và phát triển thuật toán Foldseek Cluster để so sánh mọi cấu trúc protein và tìm ra sự tương đồng giữa các protein của các loài khác nhau. Thuật toán Foldseek Cluster đã gom cụm khoảng 2,3 triệu cụm cấu trúc protein có sự tương đồng về các hình dạng ba chiều. Theo ước tính, thì thuật toán Foldseek Cluster chỉ mất vài ngày hay vài tháng để thực hiện việc gom cụm các cấu trúc protein. Công trình này ngoài việc thực hiện gom cụm hiệu quả mà còn tăng hiểu biết mới vào lịch sử tiến hoá của các protein. Các tác giả đã tìm ra những tương đồng cấu trúc giữa các protein miễn dịch người và những protein tương tự trong vi khuẩn. Ví dụ này cho thấy việc sử dụng máy tính, thuật toán đã thể hiện tính ưu việt về tốc độ, độ chính xác, khả năng mở rộng,... trong xử lí dữ liệu lớn.

Thảo luận với bạn và cho biết một số ưu điểm khi sử dụng máy tính và thuật toán để xử lí dữ liệu lớn.

Sử dụng máy tính và thuật toán trong xử lí dữ liệu lớn có nhiều ưu điểm như tăng tốc độ xử lí, khả năng tự động hoá, tính đa dạng, tính chính xác, khả năng mở rộng, khả năng lưu trữ, tiết kiệm thời gian,...

### LUYỆN TẬP

1. Trình bày các yếu tố chính của dữ liệu lớn.

2. Nêu ví dụ minh hoạ tính ưu việt của máy tính khi xử lí dữ liệu lớn.

### VẬN DỤNG

1. Cho biết dữ liệu tạo ra từ mạng xã hội có các tính chất của dữ liệu lớn không.

2. Tìm hiểu và trình bày sơ lược vai trò của tự động hoá trong Khoa học dữ liệu.
